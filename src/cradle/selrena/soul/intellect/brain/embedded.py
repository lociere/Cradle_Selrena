import asyncio
import os
from typing import List, Dict, Any
from cradle.core.config_manager import global_config
from cradle.schemas.configs.soul import LLMConfig
from cradle.utils.logger import logger
from . import BaseBrainBackend

# æ‡’åŠ è½½ï¼šé¿å…åœ¨æ¨¡å—å¯¼å…¥æ—¶å°±æ£€æŸ¥ä¾èµ–
try:
    from llama_cpp import Llama
    import llama_cpp
except ImportError:
    Llama = None
    llama_cpp = None

class LlamaCppEmbeddedBackend(BaseBrainBackend):
    """
    æœ¬åœ°å†…åµŒç¥ç»åç«¯ (In-Process Local LLM)
    åŸºäº llama.cppï¼Œåœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œæ¨ç†ï¼Œé€šè¿‡ asyncio.to_thread ä¿æŒä¸»å¾ªç¯æµç•…ã€‚
    """
    def __init__(self, config: LLMConfig):
        super().__init__(config)
        self._llm = None
        self._lock = asyncio.Lock() # ç®€å•é˜²å¹¶å‘ï¼Œç¡®ä¿åŒä¸€æ—¶åˆ»åªæœ‰ä¸€ä¸ªæ€è€ƒåœ¨è¿›è¡Œ

    async def initialize(self):
        if Llama is None:
            raise ImportError(
                "æœªæ£€æµ‹åˆ° 'llama_cpp' åº“ã€‚å¦‚éœ€ä½¿ç”¨æœ¬åœ°å†…åµŒæ¨¡å¼ï¼Œè¯·å®‰è£…: "
                "pip install llama-cpp-python (æ¨èé…åˆ GPU ä½¿ç”¨)"
            )

        # ä½¿ç”¨ ModelManager ç»Ÿä¸€è§£æ/éªŒè¯æ¨¡å‹è·¯å¾„ï¼ˆæœ¬åœ°ä¼˜å…ˆï¼›ä¸è‡ªåŠ¨ä¸‹è½½ï¼‰
        from cradle.core.model_manager import global_model_manager
        model_path = global_model_manager.resolve_model_path(self.config.local_model_path, auto_download=False)
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"[Local Backend] æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {model_path} â€” è¯·ä½¿ç”¨ ModelManager ä¸‹è½½æˆ–åœ¨é…ç½®ä¸­æä¾›æ­£ç¡®è·¯å¾„")

        logger.info(f"[Local Backend] æ­£åœ¨åŠ è½½ç¥ç»ç½‘ç»œæƒé‡... (GPU Layers: {self.config.n_gpu_layers})")
        
        # è¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œ(1-5ç§’)ï¼Œåœ¨å¯åŠ¨é˜¶æ®µæˆ‘ä»¬å¯ä»¥æ¥å—å®ƒé˜»å¡ä¸»çº¿ç¨‹
        # å¦‚æœè¿½æ±‚æè‡´å¯åŠ¨é€Ÿåº¦ï¼Œä¹Ÿå¯ä»¥æ”¾åˆ°çº¿ç¨‹é‡Œå»åˆå§‹åŒ–
        try:
            import llama_cpp
            logger.debug(f"[Local Backend] verify llama_cpp path: {llama_cpp.__file__}")

            # --- åŠ¨æ€åŠ è½½ç­–ç•¥ (Dynamic Loading Strategy) ---
            # ä¼˜å…ˆå°è¯• "æé€Ÿæ¨¡å¼" (-1)ï¼Œå¦‚æœæ˜¾å­˜(VRAM)ä¸è¶³ï¼Œè‡ªåŠ¨å›é€€åˆ° "å…¼å®¹æ¨¡å¼" (20)
            
            load_strategies = [
                {
                    "name": "ğŸš€ æé€Ÿæ¨¡å¼ (Full GPU)",
                    "kwargs": {
                        "n_gpu_layers": -1,
                        "flash_attn": True,
                        "type_k": llama_cpp.GGML_TYPE_Q8_0, 
                        "type_v": llama_cpp.GGML_TYPE_Q8_0
                    }
                },
                {
                    "name": "ğŸ›¡ï¸ å…¼å®¹æ¨¡å¼ (Partial GPU)",
                    "kwargs": {
                        "n_gpu_layers": 20, 
                        "flash_attn": False, 
                    }
                }
            ]

            is_debug = global_config.get_system().app.debug
            last_error = None
            for strategy in load_strategies:
                try:
                    logger.debug(f"[Local Backend] å°è¯•åŠ è½½ç­–ç•¥: {strategy['name']}")
                    self._llm = Llama(
                        model_path=model_path,
                        n_ctx=self.config.n_ctx,
                        verbose=is_debug,
                        **strategy["kwargs"]
                    )
                    logger.info(f"[Local Backend] æ¨¡å‹åŠ è½½æˆåŠŸ! | Context: {self.config.n_ctx} | Strategy: {strategy['name']}")
                    break # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯
                except Exception as e:
                    logger.warning(f"[Local Backend] ç­–ç•¥ {strategy['name']} åŠ è½½å¤±è´¥: {e}")
                    last_error = e
                    # æ¸…ç†åˆšæ‰å¤±è´¥çš„å®ä¾‹ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œé‡Šæ”¾æ˜¾å­˜
                    if self._llm:
                        del self._llm
                        self._llm = None
                    continue
            
            if self._llm is None:
                 logger.critical(f"[Local Backend] æ‰€æœ‰åŠ è½½ç­–ç•¥å‡å¤±è´¥ã€‚æœ€åä¸€æ¬¡é”™è¯¯: {last_error}")
                 raise last_error

        except Exception as e:
            logger.critical(f"[Local Backend] åˆå§‹åŒ–æµç¨‹å´©æºƒ: {e}")
            raise

    async def generate(self, messages: List[Dict[str, str]]) -> str:
        """
        æ‰§è¡Œéé˜»å¡æ¨ç†
        """
        if not self._llm:
            return "ï¼ˆå¤§è„‘æœªåˆå§‹åŒ–ï¼‰"

        async with self._lock:
            # ä½¿ç”¨ asyncio.to_thread å°†åŒæ­¥çš„ C++ è°ƒç”¨å‰¥ç¦»å‡ºå»
            # è¿™é‡Œçš„ create_chat_completion æ˜¯ CPU/GPU å¯†é›†å‹ä»»åŠ¡ï¼Œä¼šé‡Šæ”¾ GIL
            try:
                response = await asyncio.to_thread(
                    self._llm.create_chat_completion,
                    messages=messages,
                    temperature=self.config.temperature,
                    max_tokens=self.config.max_tokens,
                    stream=False # æš‚ä¸ä½¿ç”¨æµå¼ï¼Œä¸ºäº†æ¶æ„ç®€å•
                )
                return response['choices'][0]['message']['content']
            except Exception as e:
                logger.error(f"[Local Backend] çªè§¦è¿æ¥ä¸­æ–­ (Inference Error): {e}")
                return "ï¼ˆ...æ€è€ƒè¢«æ‰“æ–­äº†ï¼‰"

    async def cleanup(self):
        # llama_cpp å¯¹è±¡ææ„æ—¶ä¼šè‡ªåŠ¨é‡Šæ”¾æ˜¾å­˜
        self._llm = None
        logger.info("[Local Backend] ç¥ç»ç½‘ç»œå·²å¸è½½ï¼Œæ˜¾å­˜å·²é‡Šæ”¾ã€‚")
